_wandb:
    value:
        cli_version: 0.21.3
        e:
            bljor16n89t0724on0i35f29g2djhufq:
                args:
                    - --wandb
                codePath: isaac_env/env/train.py
                codePathLocal: isaac_env/env/train.py
                cpu_count: 10
                cpu_count_logical: 10
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "983247642624"
                        used: "323845763072"
                email: pakbohyong0717@gmail.com
                executable: /home/home/isaacsim/_build/linux-x86_64/release/kit/python/bin/python3
                gpu: NVIDIA GeForce RTX 5070
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Blackwell
                      cudaCores: 6144
                      memoryTotal: "12820938752"
                      name: NVIDIA GeForce RTX 5070
                      uuid: GPU-c450e067-8610-e678-c4cb-409ef308a10f
                host: home
                memory:
                    total: "33259450368"
                os: Linux-6.8.0-79-generic-x86_64-with-glibc2.35
                program: /home/home/soarm_tutorial/isaac_env/env/train.py
                python: CPython 3.11.12
                root: /home/home/soarm_tutorial
                startedAt: "2025-09-08T08:47:06.148414Z"
                writerId: bljor16n89t0724on0i35f29g2djhufq
        m: []
        python_version: 3.11.12
        t:
            "1":
                - 1
            "2":
                - 1
            "3":
                - 13
                - 16
            "4": 3.11.12
            "5": 0.21.3
            "12": 0.21.3
            "13": linux-x86_64
env:
    value:
        max_steps: 100
        render: true
model:
    value:
        hidden_sizes:
            - 128
            - 64
        save_dir: /home/home/soarm_tutorial/policy/PPO/checkpoints
        save_name: ppo_soarm.pth
train:
    value:
        batch_size: 64
        clip_eps: 0.2
        epochs: 10
        gamma: 0.99
        lam: 0.95
        learning_rate: "3e-4"
        rollout_size: 2048
        save_interval: 50
        total_episodes: 1000
